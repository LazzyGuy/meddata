


<PubmedArticle>
    <MedlineCitation Status="Publisher" Owner="NLM">
        <PMID Version="1">31199252</PMID>
        <DateRevised>
            <Year>2019</Year>
            <Month>06</Month>
            <Day>14</Day>
        </DateRevised>
        <Article PubModel="Print-Electronic">
            <Journal>
                <ISSN IssnType="Electronic">1939-3539</ISSN>
                <JournalIssue CitedMedium="Internet">
                    <PubDate>
                        <Year>2019</Year>
                        <Month>Jun</Month>
                        <Day>11</Day>
                    </PubDate>
                </JournalIssue>
                <Title>IEEE transactions on pattern analysis and machine intelligence</Title>
                <ISOAbbreviation>IEEE Trans Pattern Anal Mach Intell</ISOAbbreviation>
            </Journal>
            <ArticleTitle>Object Detection from Scratch with Deep Supervision.</ArticleTitle>
            <ELocationID EIdType="doi" ValidYN="Y">10.1109/TPAMI.2019.2922181</ELocationID>
            <Abstract>
                <AbstractText>We propose Deeply Supervised Object Detectors (DSOD), an object detection framework that can be trained from scratch. Recent advances in object detection heavily depend on the off-the-shelf models pre-trained on large-scale classification datasets like ImageNet and OpenImage. However, one problem is that adopting pre-trained models from classification to detection task may incur learning bias due to the different objective function and diverse distributions of object categories. Techniques like fine-tuning on detection task could alleviate this issue to some extent but are still not fundamental. Furthermore, transferring these pre-trained models cross discrepant domains will be more difficult (e.g., from RGB to depth images). Thus, a better solution to handle these critical problems is to train object detectors from scratch, which motivates our proposed method. In DSOD, we contribute a set of design principles for learning object detectors from scratch. One of the key principles is the deep supervision, enabled by layer-wise dense connections in both backbone networks and prediction layers, plays a critical role in learning good detectors from scratch. We evaluate our method on PASCAL VOC 2007, 2012 and COCO datasets. DSOD achieves consistently better results than the state-of-the-art methods with much more compact models.</AbstractText>
            </Abstract>
            <AuthorList CompleteYN="Y">
                <Author ValidYN="Y">
                    <LastName>Shen</LastName>
                    <ForeName>Zhiqiang</ForeName>
                    <Initials>Z</Initials>
                </Author>
                <Author ValidYN="Y">
                    <LastName>Liu</LastName>
                    <ForeName>Zhuang</ForeName>
                    <Initials>Z</Initials>
                </Author>
                <Author ValidYN="Y">
                    <LastName>Li</LastName>
                    <ForeName>Jianguo</ForeName>
                    <Initials>J</Initials>
                </Author>
                <Author ValidYN="Y">
                    <LastName>Jiang</LastName>
                    <ForeName>Yu-Gang</ForeName>
                    <Initials>YG</Initials>
                </Author>
                <Author ValidYN="Y">
                    <LastName>Chen</LastName>
                    <ForeName>Yurong</ForeName>
                    <Initials>Y</Initials>
                </Author>
                <Author ValidYN="Y">
                    <LastName>Xue</LastName>
                    <ForeName>Xiangyang</ForeName>
                    <Initials>X</Initials>
                </Author>
            </AuthorList>
            <Language>eng</Language>
            <PublicationTypeList>
                <PublicationType UI="D016428">Journal Article</PublicationType>
            </PublicationTypeList>
            <ArticleDate DateType="Electronic">
                <Year>2019</Year>
                <Month>06</Month>
                <Day>11</Day>
            </ArticleDate>
        </Article>
        <MedlineJournalInfo>
            <Country>United States</Country>
            <MedlineTA>IEEE Trans Pattern Anal Mach Intell</MedlineTA>
            <NlmUniqueID>9885960</NlmUniqueID>
            <ISSNLinking>0098-5589</ISSNLinking>
        </MedlineJournalInfo>
    </MedlineCitation>
    <PubmedData>
        <History>
            <PubMedPubDate PubStatus="entrez">
                <Year>2019</Year>
                <Month>6</Month>
                <Day>15</Day>
                <Hour>6</Hour>
                <Minute>0</Minute>
            </PubMedPubDate>
            <PubMedPubDate PubStatus="pubmed">
                <Year>2019</Year>
                <Month>6</Month>
                <Day>15</Day>
                <Hour>6</Hour>
                <Minute>0</Minute>
            </PubMedPubDate>
            <PubMedPubDate PubStatus="medline">
                <Year>2019</Year>
                <Month>6</Month>
                <Day>15</Day>
                <Hour>6</Hour>
                <Minute>0</Minute>
            </PubMedPubDate>
        </History>
        <PublicationStatus>aheadofprint</PublicationStatus>
        <ArticleIdList>
            <ArticleId IdType="pubmed">31199252</ArticleId>
            <ArticleId IdType="doi">10.1109/TPAMI.2019.2922181</ArticleId>
        </ArticleIdList>
    </PubmedData>
</PubmedArticle>

