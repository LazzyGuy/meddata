


<PubmedArticle>
    <MedlineCitation Status="Publisher" Owner="NLM">
        <PMID Version="1">31199271</PMID>
        <DateRevised>
            <Year>2019</Year>
            <Month>06</Month>
            <Day>14</Day>
        </DateRevised>
        <Article PubModel="Print-Electronic">
            <Journal>
                <ISSN IssnType="Electronic">2162-2388</ISSN>
                <JournalIssue CitedMedium="Internet">
                    <PubDate>
                        <Year>2019</Year>
                        <Month>Jun</Month>
                        <Day>12</Day>
                    </PubDate>
                </JournalIssue>
                <Title>IEEE transactions on neural networks and learning systems</Title>
                <ISOAbbreviation>IEEE Trans Neural Netw Learn Syst</ISOAbbreviation>
            </Journal>
            <ArticleTitle>Adaptive Deep Modeling of Users and Items Using Side Information for Recommendation.</ArticleTitle>
            <ELocationID EIdType="doi" ValidYN="Y">10.1109/TNNLS.2019.2909432</ELocationID>
            <Abstract>
                <AbstractText>In the existing recommender systems, matrix factorization (MF) is widely applied to model user preferences and item features by mapping the user-item ratings into a low-dimension latent vector space. However, MF has ignored the individual diversity where the user's preference for different unrated items is usually different. A fixed representation of user preference factor extracted by MF cannot model the individual diversity well, which leads to a repeated and inaccurate recommendation. To this end, we propose a novel latent factor model called adaptive deep latent factor model (ADLFM), which learns the preference factor of users adaptively in accordance with the specific items under consideration. We propose a novel user representation method that is derived from their rated item descriptions instead of original user-item ratings. Based on this, we further propose a deep neural networks framework with an attention factor to learn the adaptive representations of users. Extensive experiments on Amazon data sets demonstrate that ADLFM outperforms the state-of-the-art baselines greatly. Also, further experiments show that the attention factor indeed makes a great contribution to our method.</AbstractText>
            </Abstract>
            <AuthorList CompleteYN="Y">
                <Author ValidYN="Y">
                    <LastName>Han</LastName>
                    <ForeName>Jiayu</ForeName>
                    <Initials>J</Initials>
                </Author>
                <Author ValidYN="Y">
                    <LastName>Zheng</LastName>
                    <ForeName>Lei</ForeName>
                    <Initials>L</Initials>
                </Author>
                <Author ValidYN="Y">
                    <LastName>Xu</LastName>
                    <ForeName>Yuanbo</ForeName>
                    <Initials>Y</Initials>
                </Author>
                <Author ValidYN="Y">
                    <LastName>Zhang</LastName>
                    <ForeName>Bangzuo</ForeName>
                    <Initials>B</Initials>
                </Author>
                <Author ValidYN="Y">
                    <LastName>Zhuang</LastName>
                    <ForeName>Fuzhen</ForeName>
                    <Initials>F</Initials>
                </Author>
                <Author ValidYN="Y">
                    <LastName>Yu</LastName>
                    <ForeName>Philip S</ForeName>
                    <Initials>PS</Initials>
                </Author>
                <Author ValidYN="Y">
                    <LastName>Zuo</LastName>
                    <ForeName>Wanli</ForeName>
                    <Initials>W</Initials>
                </Author>
            </AuthorList>
            <Language>eng</Language>
            <PublicationTypeList>
                <PublicationType UI="D016428">Journal Article</PublicationType>
            </PublicationTypeList>
            <ArticleDate DateType="Electronic">
                <Year>2019</Year>
                <Month>06</Month>
                <Day>12</Day>
            </ArticleDate>
        </Article>
        <MedlineJournalInfo>
            <Country>United States</Country>
            <MedlineTA>IEEE Trans Neural Netw Learn Syst</MedlineTA>
            <NlmUniqueID>101616214</NlmUniqueID>
            <ISSNLinking>2162-237X</ISSNLinking>
        </MedlineJournalInfo>
    </MedlineCitation>
    <PubmedData>
        <History>
            <PubMedPubDate PubStatus="entrez">
                <Year>2019</Year>
                <Month>6</Month>
                <Day>15</Day>
                <Hour>6</Hour>
                <Minute>0</Minute>
            </PubMedPubDate>
            <PubMedPubDate PubStatus="pubmed">
                <Year>2019</Year>
                <Month>6</Month>
                <Day>15</Day>
                <Hour>6</Hour>
                <Minute>0</Minute>
            </PubMedPubDate>
            <PubMedPubDate PubStatus="medline">
                <Year>2019</Year>
                <Month>6</Month>
                <Day>15</Day>
                <Hour>6</Hour>
                <Minute>0</Minute>
            </PubMedPubDate>
        </History>
        <PublicationStatus>aheadofprint</PublicationStatus>
        <ArticleIdList>
            <ArticleId IdType="pubmed">31199271</ArticleId>
            <ArticleId IdType="doi">10.1109/TNNLS.2019.2909432</ArticleId>
        </ArticleIdList>
    </PubmedData>
</PubmedArticle>

